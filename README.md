# SAM3 Unified Pipeline

Self-contained multi-class object detection with pixel-level masks.
Supports **text**, **image (geometry)**, or **both** prompt types per class.

> **First time?** Start with [`setup/`](setup/) — it has everything you need to build the
> Docker environment and TensorRT engines from scratch. See [`setup/README.md`](setup/README.md).

## Folder Structure

```
sam3_pipeline/
├── setup/                    # Environment setup & engine building (run first!)
│   ├── README.md             #   Step-by-step guide (from zero)
│   ├── export_sam3_to_onnx.py #  PyTorch → ONNX export script
│   └── onnx_to_tensorrt.sh   #  ONNX → TensorRT engine builder
├── engines/                  # TensorRT engines + tokenizer (self-contained)
│   ├── vision-encoder.engine
│   ├── text-encoder.engine
│   ├── geometry-encoder.engine
│   ├── decoder.engine
│   └── tokenizer.json
├── features/                 # Auto-generated by extract.py
│   ├── {class_name}/
│   │   ├── features.npy      # [1, prompt_len, 256] float32
│   │   ├── mask.npy           # [1, prompt_len] bool
│   │   └── meta.json
│   └── _meta.json
├── Inputs/                   # Source images & videos for inference
├── references/               # Reference images for image prompts
├── outputs/                  # Detection results
├── config.json               # Class definitions
├── extract.py                # Step 1: pre-compute prompt features
└── infer.py                  # Step 2: run detection
```

## Prerequisites

**Before running the pipeline**, you need TensorRT engines and a Docker environment.
If you don't have these yet, follow the guide in [`setup/README.md`](setup/README.md) first.

## Quick Start

All commands run inside the `sam3_trt` Docker container (see [`setup/`](setup/) to create it).

### Step 1: Configure classes

Edit `config.json`:

```json
{
  "engines": "engines",
  "tokenizer": "engines/tokenizer.json",
  "features": "features",
  "confidence": 0.3,
  "classes": [
    {"name": "person", "prompt_type": "text", "text": "person"},
    {"name": "hand",   "prompt_type": "text", "text": "hand"}
  ]
}
```

### Step 2: Extract prompt features

```bash
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/extract.py \
  --config /root/VisionDSL/models/sam3_pipeline/config.json
```

This runs once per config change. Unchanged classes are cached automatically.

### Step 3: Run inference

**Single image:**

```bash
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/infer.py \
  --config /root/VisionDSL/models/sam3_pipeline/config.json \
  --images /root/VisionDSL/models/sam3_pipeline/Inputs/demo_3.jpg \
  --output /root/VisionDSL/models/sam3_pipeline/outputs
```

**Video (adaptive real-time):**

```bash
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/infer.py \
  --config /root/VisionDSL/models/sam3_pipeline/config.json \
  --video /root/VisionDSL/models/sam3_pipeline/Inputs/media1.mp4 \
  --output /root/VisionDSL/models/sam3_pipeline/outputs
```

Video mode simulates real-time playback: after each inference, the video clock
advances by the actual inference time. Frames that arrive while the GPU is busy
are dropped. The output AVI plays at the exact achieved fps, matching the
original video duration with zero drift.

## Output Files

Each run produces timestamped output files (`YYYYMMDD_HHMMSS` prefix), so multiple runs don't overwrite each other.

**Image mode:**

| File | Description |
|------|-------------|
| `{name}_overlay.jpg` | Image with coloured mask overlays + labels |
| `{name}_mask_{class}.png` | Binary mask per class (only with `--masks`) |
| `{timestamp}_detections.jsonl` | Per-image detections (streaming JSONL) |
| `{timestamp}_performance.json` | Timing stats (avg/min/max/p95 ms, estimated FPS) |

**Video mode:**

| File | Description |
|------|-------------|
| `{timestamp}_output.avi` | MJPG video with overlay frames |
| `{timestamp}_detections.jsonl` | Per-frame detections (streaming JSONL, `tail -f` compatible) |
| `{timestamp}_performance.json` | Timing stats |

## CLI Flags

| Flag | Default | Description |
|------|---------|-------------|
| `--config` | (required) | Path to config.json |
| `--images` | — | Image file paths (mutually exclusive with --video) |
| `--video` | — | Video file path |
| `--output` | `outputs` | Output directory |
| `--conf` | from config | Confidence threshold override |
| `--interval` | `1` | Min frame gap (video); 1=GPU-adaptive, N=at most every Nth frame |
| `--masks` | `false` | Save per-class mask PNGs |

## Prompt Types

### Text prompt

Describe the object with a word or phrase. Uses the text encoder.

```json
{"name": "person", "prompt_type": "text", "text": "person"}
```

### Image prompt (geometry)

Point to the object in a reference image using bounding boxes.
Boxes are in **normalised cxcywh** format: `[center_x, center_y, width, height]`, values 0.0–1.0.
Labels: `1` = positive (this is the object), `0` = negative (not this).

```json
{
  "name": "my_cup",
  "prompt_type": "image",
  "references": [
    {"image": "references/cup.jpg", "boxes": [[0.5, 0.5, 0.3, 0.4]], "labels": [1]}
  ]
}
```

### Combined prompt (both)

Text description + reference image. Features are concatenated.

```json
{
  "name": "red_cup",
  "prompt_type": "both",
  "text": "cup",
  "references": [
    {"image": "references/red_cup.jpg", "boxes": [[0.5, 0.5, 0.3, 0.4]], "labels": [1]}
  ]
}
```

## Limits

| Constraint | Value |
|------------|-------|
| Max classes | 4 (decoder batch dimension) |
| Max prompt tokens | 60 (text=32, each geo box=2 tokens) |
| Max geo boxes per class | 20 |

## Performance (benchmarked)

| Classes | Avg ms/frame | Est. FPS |
|---------|-------------|----------|
| 1 (text) | ~50 ms | ~20 |
| 2 (text) | ~53 ms | ~17 |

First frame includes warmup and is slower (~100–350 ms).

## Two-Step Workflow

```
config.json  ──>  extract.py  ──>  features/   ──>  infer.py  ──>  outputs/
 (define)          (once)        (cached)          (per frame)     (results)
```

Only `extract.py` needs the text/geometry encoders. `infer.py` only loads the
vision encoder + decoder, making it fast and lightweight.
