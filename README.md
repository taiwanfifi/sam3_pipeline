# SAM3 Unified Pipeline

Self-contained multi-class object detection with pixel-level masks.
Supports **text**, **image (geometry)**, or **both** prompt types per class.

> **First time?** Start with [`setup/`](setup/) — it has everything you need to build the
> Docker environment and TensorRT engines from scratch. See [`setup/README.md`](setup/README.md).

## Folder Structure

```
sam3_pipeline/
├── setup/                    # Environment setup & engine building (run first!)
│   ├── README.md             #   Step-by-step guide (from zero)
│   ├── export_sam3_to_onnx.py #  PyTorch → ONNX export script
│   └── onnx_to_tensorrt.sh   #  ONNX → TensorRT engine builder
├── engines/                  # TensorRT engines (organised by variant)
│   ├── b8_q200/              #   batch=8, queries=200 (default)
│   ├── b8_q50/               #   batch=8, queries=50 (VRAM optimised)
│   ├── b4_q200/              #   batch=4, queries=200 (low VRAM)
│   └── tokenizer.json
├── features/                 # Auto-generated by extract.py
│   ├── {class_name}/
│   │   ├── features.npy      # [1, prompt_len, 256] float32
│   │   ├── mask.npy           # [1, prompt_len] bool
│   │   └── meta.json
│   └── _meta.json
├── Inputs/                   # Source images & videos for inference
├── references/               # Reference images for image prompts
├── outputs/                  # Detection results
├── config.json               # Class definitions (default: b8_q200 engines)
├── config_q50.json           # Same classes, pointing to b8_q50 engines
├── config_editor.py          # Visual config editor (standalone, see below)
├── extract.py                # Step 1: pre-compute prompt features
├── infer.py                  # Step 2: run detection (single camera)
├── infer_multi.py            # Step 2 alt: multi-camera pipeline (8 cameras)
└── optimize.md               # QUERIES optimization experiment & results
```

## Prerequisites

**Before running the pipeline**, you need TensorRT engines and a Docker environment.
If you don't have these yet, follow the guide in [`setup/README.md`](setup/README.md) first.

## Quick Start

All commands run inside the `sam3_trt` Docker container (see [`setup/`](setup/) to create it).

### Step 1: Configure classes

Edit `config.json` by hand, or use the visual editor (see [Config Editor](#config-editor) below):

```json
{
  "engines": "engines/b8_q200",
  "tokenizer": "engines/tokenizer.json",
  "features": "features",
  "confidence": 0.3,
  "classes": [
    {"name": "person", "prompt_type": "text", "text": "person"},
    {"name": "hand",   "prompt_type": "text", "text": "hand"}
  ]
}
```

Switch to the VRAM-optimised engine by changing `"engines"` to `"engines/b8_q50"`. See [`optimize.md`](optimize.md) for details.

### Step 2: Extract prompt features

```bash
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/extract.py \
  --config /root/VisionDSL/models/sam3_pipeline/config.json
```

This runs once per config change. Unchanged classes are cached automatically.

### Step 3: Run inference

**Single image:**

```bash
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/infer.py \
  --config /root/VisionDSL/models/sam3_pipeline/config.json \
  --images /root/VisionDSL/models/sam3_pipeline/Inputs/demo_3.jpg \
  --output /root/VisionDSL/models/sam3_pipeline/outputs
```

**Video (adaptive real-time):**

```bash
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/infer.py \
  --config /root/VisionDSL/models/sam3_pipeline/config.json \
  --video /root/VisionDSL/models/sam3_pipeline/Inputs/media1.mp4 \
  --output /root/VisionDSL/models/sam3_pipeline/outputs
```

Video mode simulates real-time playback: after each inference, the video clock
advances by the actual inference time. Frames that arrive while the GPU is busy
are dropped. The output AVI plays at the exact achieved fps, matching the
original video duration with zero drift.

## Output Files

Each run produces timestamped output files (`YYYYMMDD_HHMMSS` prefix), so multiple runs don't overwrite each other.

**Image mode:**

| File | Description |
|------|-------------|
| `{name}_overlay.jpg` | Image with coloured mask overlays + labels |
| `{name}_mask_{class}.png` | Binary mask per class (only with `--masks`) |
| `{timestamp}_detections.jsonl` | Per-image detections (streaming JSONL) |
| `{timestamp}_performance.json` | Timing stats (avg/min/max/p95 ms, estimated FPS) |

**Video mode:**

| File | Description |
|------|-------------|
| `{timestamp}_output.avi` | MJPG video with overlay frames |
| `{timestamp}_detections.jsonl` | Per-frame detections (streaming JSONL, `tail -f` compatible) |
| `{timestamp}_performance.json` | Timing stats |

## CLI Flags

| Flag | Default | Description |
|------|---------|-------------|
| `--config` | (required) | Path to config.json |
| `--images` | — | Image file paths (mutually exclusive with --video) |
| `--video` | — | Video file path |
| `--output` | `outputs` | Output directory |
| `--conf` | from config | Confidence threshold override |
| `--interval` | `1` | Min frame gap (video); 1=GPU-adaptive, N=at most every Nth frame |
| `--masks` | `false` | Save per-class mask PNGs |

## Prompt Types

### Text prompt

Describe the object with a word or phrase. Uses the text encoder.

```json
{"name": "person", "prompt_type": "text", "text": "person"}
```

### Image prompt (geometry)

Point to the object in a reference image using bounding boxes.
Boxes are in **normalised cxcywh** format: `[center_x, center_y, width, height]`, values 0.0–1.0.
Labels: `1` = positive (this is the object), `0` = negative (not this).

```json
{
  "name": "my_cup",
  "prompt_type": "image",
  "references": [
    {"image": "references/cup.jpg", "boxes": [[0.5, 0.5, 0.3, 0.4]], "labels": [1]}
  ]
}
```

### Combined prompt (both)

Text description + reference image. Features are concatenated.

```json
{
  "name": "red_cup",
  "prompt_type": "both",
  "text": "cup",
  "references": [
    {"image": "references/red_cup.jpg", "boxes": [[0.5, 0.5, 0.3, 0.4]], "labels": [1]}
  ]
}
```

## Limits

| Constraint | Value |
|------------|-------|
| Max classes | 8 (decoder batch dimension; adjustable, see [`setup/README.md`](setup/README.md)) |
| Max prompt tokens | 60 (text=32, each geo box=2 tokens) |
| Max geo boxes per class | 20 |

## Performance & VRAM (benchmarked on RTX 5090)

### Single camera (`infer.py`)

| Classes | Engine variant | Avg ms/frame | Est. FPS | VRAM |
|---------|---------------|-------------|----------|------|
| 4 (3 text + 1 image) | b4_q200 | ~70 ms | ~14 | **~5.0 GB** |
| 4 (3 text + 1 image) | b8_q200 | ~70 ms | ~14 | **~7.5 GB** |

### Multi-camera (`infer_multi.py`, 8 cameras)

| Variant | Avg ms/round | Per camera | Throughput | VRAM |
|---------|-------------|-----------|-----------|------|
| b8_q200 | 426 ms | 53 ms | 18.8 FPS | **8.7 GB** |
| b8_q50 | 425 ms | 53 ms | 18.8 FPS | **7.5 GB** |

Q50 saves ~1.2 GB VRAM with zero speed/quality loss. See [`optimize.md`](optimize.md) for full analysis.

- First frame includes warmup and is slower (~100–350 ms)
- VRAM is dominated by TensorRT activation memory, pre-allocated for `maxShapes` at engine load time
- `QUERIES` is auto-detected from the decoder engine — no manual configuration needed
- To change the max class limit, rebuild engines with a different batch size (see [`setup/README.md`](setup/README.md))

## Multi-Camera Mode (`infer_multi.py`)

For processing multiple camera feeds simultaneously (Plan C v3 architecture).

**Single video (duplicated across all cameras):**

```bash
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/infer_multi.py \
  --config /root/VisionDSL/models/sam3_pipeline/config.json \
  --video /root/VisionDSL/models/sam3_pipeline/Inputs/media1.mp4 \
  --cameras 8 \
  --output /root/VisionDSL/models/sam3_pipeline/outputs
```

**Multiple videos (cycled to fill camera slots):**

```bash
# 3 videos → 8 cameras: shop,hair,car,shop,hair,car,shop,hair
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/infer_multi.py \
  --config /root/VisionDSL/models/sam3_pipeline/config_q50.json \
  --video Inputs/shop.mp4 Inputs/hair.mp4 Inputs/car.mp4 \
  --cameras 8

# 8 unique videos (production scenario)
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/infer_multi.py \
  --config /root/VisionDSL/models/sam3_pipeline/config_q50.json \
  --video cam1.mp4 cam2.mp4 cam3.mp4 cam4.mp4 cam5.mp4 cam6.mp4 cam7.mp4 cam8.mp4 \
  --cameras 8
```

**With grid overlay video (for visual review):**

```bash
docker exec william_tensorrt python3 \
  /root/VisionDSL/models/sam3_pipeline/infer_multi.py \
  --config /root/VisionDSL/models/sam3_pipeline/config_q50.json \
  --video Inputs/shop.mp4 Inputs/hair.mp4 Inputs/car.mp4 \
  --cameras 8 --save-video
```

This produces a 2x4 grid AVI (`{timestamp}_grid.avi`) with all 8 camera overlays side by side. Slower than detection-only mode — use without `--save-video` for benchmarking.

Key optimisations:
- **VE batch=F**: all camera frames in one vision encoder pass
- **Zero-copy FPN**: VE output buffers directly used as decoder input
- **Decoder iterates per class**: each with batch=F (all frames)
- **Double-buffered decoder output**: decoder N+1 overlaps mask copy N
- **Selective mask copy**: only transfer masks for detected objects (~40x less PCIe bandwidth)
- **Auto-detection**: IMAGE_SIZE and QUERIES read from engine files — no manual config

| Flag | Default | Description |
|------|---------|-------------|
| `--config` | (required) | Path to config.json |
| `--video` | (required) | Video file(s); cycled if fewer than `--cameras` |
| `--cameras` | `8` | Number of camera slots |
| `--output` | `outputs` | Output directory |
| `--conf` | from config | Confidence threshold override |
| `--interval` | `1` | Min frame gap |
| `--save-video` | `false` | Save 2x4 grid overlay AVI for visual review |

## Two-Step Workflow

```
config.json  ──>  extract.py  ──>  features/   ──>  infer.py  ──>  outputs/
 (define)          (once)        (cached)          (per frame)     (results)
```

Only `extract.py` needs the text/geometry encoders. `infer.py` only loads the
vision encoder + decoder, making it fast and lightweight.

## Config Editor

A standalone visual tool for editing `config.json`. Especially useful for **image prompts**, where you need to draw bounding boxes on reference images to get the normalised cxcywh coordinates.

- **No dependencies** — pure Python (`http.server`), single file, no pip install needed
- **No Docker required** — runs directly on the host machine
- **Does not affect the pipeline** — only reads/writes `config.json` and serves reference images

### Usage

```bash
# Run directly on host (NOT inside Docker)
python3 /home/ubuntu/Documents/willy/repos/william/VisionDSL/models/sam3_pipeline/config_editor.py \
  --config /home/ubuntu/Documents/willy/repos/william/VisionDSL/models/sam3_pipeline/config.json
```

Then open `http://localhost:8080` in your browser.

### Options

| Flag | Default | Description |
|------|---------|-------------|
| `--config` | `config.json` in script directory | Path to config.json |
| `--port` | `8080` | HTTP port |

### Features

- Add/delete classes (max 8), edit name and prompt type
- Text prompts: edit the text field directly
- Image prompts: select a reference image, click-drag to draw bounding boxes on a canvas
- Each box has a positive/negative label toggle
- Normalised cxcywh coordinates are computed automatically
- Save with button or `Ctrl+S`, dirty state tracking
- Dark theme UI
